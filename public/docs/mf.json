{
  "name" : "Matrix Factorization",
  "cells" : [ {
    "id" : 0,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val K = 2\nval N = 3\nval M = 4\n\n// set up data\n@domain case class X(i: Int, j: Int, r: Double)\nimplicit val Xs = X.Values(Ints(0 until N), Ints(0 until M), Doubles)\n\n//the set of possible parameters\n@domain case class W(u: IndexedSeq[Vect], v: IndexedSeq[Vect])\nimplicit val Ws = W.Values(Seqs(Vectors(K), N), Seqs(Vectors(K), M))\n\nX(1,2,0.0)",
      "extraFields" : {
        "aggregatedCells" : "[]"
      }
    }
  }, {
    "id" : 1,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val input = Map((0 -> 0) -> 1.0, (0 -> 1) -> 1.0, (0 -> 2) -> 0.0, (0 -> 3) -> 0.0,\n    (1 -> 0) -> 0.0, (1 -> 1) -> 0.0, (1 -> 2) -> 1.0, (1 -> 3) -> 1.0,\n    (2 -> 0) -> 1.0\n)\nval data = input.map(p => X(p._1._1, p._1._2, p._2)).toSet\nlazy val dataTerm = data.toIndexedSeq.toConst\n\n//input\n//data\nMatrix((0 until N).map(i => (0 until M).map(j => input.getOrElse(i -> j, 0.5))))",
      "extraFields" : {
        "aggregatedCells" : "[\"val K = 2\\nval N = 3\\nval M = 4\\n\\n// set up data\\n@domain case class X(i: Int, j: Int, r: Double)\\nimplicit val Xs = X.Values(Ints(0 until N), Ints(0 until M), Doubles)\\n\\n//the set of possible parameters\\n@domain case class W(u: IndexedSeq[Vect], v: IndexedSeq[Vect])\\nimplicit val Ws = W.Values(Seqs(Vectors(K), N), Seqs(Vectors(K), M))\\n\\nWs.zero.toConst\"]"
      }
    }
  }, {
    "id" : 2,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def score(w: Ws.Term)(x: Xs.Term):DoubleTerm = w.u(x.i) dot w.v(x.j)\n\ndef regularize(w: Ws.Term)(x: Xs.Term) = w.u(x.i).l2() + w.v(x.j).l2()\n\nval lambda = 0.1\n// set up learning\ndef loss(w: Ws.Term) = sum(dataTerm) {\n    x => -sq(x.r - score(w)(x)) - lambda*regularize(w)(x)\n}\nscore(Ws.zero.toConst)(X(1,2,0.0).toConst)",
      "extraFields" : {
        "cache" : "false",
        "aggregatedCells" : "[\"val K = 2\\nval N = 3\\nval M = 4\\n\\n// set up data\\n@domain case class X(i: Int, j: Int, r: Double)\\nimplicit val Xs = X.Values(Ints(0 until N), Ints(0 until M), Doubles)\\n\\n//the set of possible parameters\\n@domain case class W(u: IndexedSeq[Vect], v: IndexedSeq[Vect])\\nimplicit val Ws = W.Values(Seqs(Vectors(K), N), Seqs(Vectors(K), M))\\n\\nWs.zero.toConst\",\"val input = Map((0 -> 0) -> 1.0, (0 -> 1) -> 1.0, (0 -> 2) -> 0.0, (0 -> 3) -> 0.0,\\n    (1 -> 0) -> 0.0, (1 -> 1) -> 0.0, (1 -> 2) -> 1.0, (1 -> 3) -> 1.0,\\n    (2 -> 0) -> 0.0\\n)\\nval data = input.map(p => X(p._1._1, p._1._2, p._2)).toSet\\nlazy val dataTerm = data.toIndexedSeq.toConst\\n\\n//input\\n//data\\nMatrix((0 until N).map(i => (0 until M).map(j => input.getOrElse(i -> j, 0.5))))\"]"
      }
    }
  }, {
    "id" : 3,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "// visualize results\nval init = Settings(Ws.createRandomSetting(random.nextGaussian() * 0.1))\nval adaParams = AdaGradParameters(epochs = 100, learningRate = 0.1, initParams = init)\n\n//do the training (argmax is a term, so it needs to be evaluated to do the optimization)\nval w = argmax(Ws)(w => loss(w).argmaxBy(Argmaxer.adaGrad(adaParams))).eval()\nw",
      "extraFields" : {
        "cache" : "false",
        "aggregatedCells" : "[\"val K = 2\\nval N = 3\\nval M = 4\\n\\n// set up data\\n@domain case class X(i: Int, j: Int, r: Double)\\nimplicit val Xs = X.Values(Ints(0 until N), Ints(0 until M), Doubles)\\n\\n//the set of possible parameters\\n@domain case class W(u: IndexedSeq[Vect], v: IndexedSeq[Vect])\\nimplicit val Ws = W.Values(Seqs(Vectors(K), N), Seqs(Vectors(K), M))\\n\\nWs.zero.toConst\",\"val input = Map((0 -> 0) -> 1.0, (0 -> 1) -> 1.0, (0 -> 2) -> 0.0, (0 -> 3) -> 0.0,\\n    (1 -> 0) -> 0.0, (1 -> 1) -> 0.0, (1 -> 2) -> 1.0, (1 -> 3) -> 1.0,\\n    (2 -> 0) -> 0.0\\n)\\nval data = input.map(p => X(p._1._1, p._1._2, p._2)).toSet\\nlazy val dataTerm = data.toIndexedSeq.toConst\\n\\n//input\\n//data\\nMatrix((0 until N).map(i => (0 until M).map(j => input.getOrElse(i -> j, 0.5))))\",\"def score(w: Ws.Term)(x: Xs.Term) = w.u(x.i) dot w.v(x.j)\\n\\ndef regularize(w: Ws.Term)(x: Xs.Term) = w.u(x.i).l2() + w.v(x.j).l2()\\n\\nval lambda = 0.1\\n// set up learning\\ndef loss(w: Ws.Term) = sum(dataTerm) {\\n    x => -sq(x.r - score(w)(x)) - lambda*regularize(w)(x)\\n}\\nscore(Ws.zero.toConst)(X(0,0,1.0).toConst)\"]"
      }
    }
  }, {
    "id" : 4,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "// predict\ndef predict(person: Int, movie: Int) = score(w.toConst)(X(person, movie, 0.0).toConst).eval()\nMatrix((0 until N).map(i => (0 until M).map(j => predict(i, j))))",
      "extraFields" : {
        "cache" : "false",
        "aggregatedCells" : "[\"val K = 2\\nval N = 3\\nval M = 4\\n\\n// set up data\\n@domain case class X(i: Int, j: Int, r: Double)\\nimplicit val Xs = X.Values(Ints(0 until N), Ints(0 until M), Doubles)\\n\\n//the set of possible parameters\\n@domain case class W(u: IndexedSeq[Vect], v: IndexedSeq[Vect])\\nimplicit val Ws = W.Values(Seqs(Vectors(K), N), Seqs(Vectors(K), M))\\n\\nWs.zero.toConst\",\"val input = Map((0 -> 0) -> 1.0, (0 -> 1) -> 1.0, (0 -> 2) -> 0.0, (0 -> 3) -> 0.0,\\n    (1 -> 0) -> 0.0, (1 -> 1) -> 0.0, (1 -> 2) -> 1.0, (1 -> 3) -> 1.0,\\n    (2 -> 0) -> 0.0\\n)\\nval data = input.map(p => X(p._1._1, p._1._2, p._2)).toSet\\nlazy val dataTerm = data.toIndexedSeq.toConst\\n\\n//input\\n//data\\nMatrix((0 until N).map(i => (0 until M).map(j => input.getOrElse(i -> j, 0.5))))\",\"def score(w: Ws.Term)(x: Xs.Term) = w.u(x.i) dot w.v(x.j)\\n\\ndef regularize(w: Ws.Term)(x: Xs.Term) = w.u(x.i).l2() + w.v(x.j).l2()\\n\\nval lambda = 0.1\\n// set up learning\\ndef loss(w: Ws.Term) = sum(dataTerm) {\\n    x => -sq(x.r - score(w)(x)) - lambda*regularize(w)(x)\\n}\\nscore(Ws.zero.toConst)(X(0,0,1.0).toConst)\",\"// visualize results\\nval init = Settings(Ws.createRandomSetting(random.nextGaussian() * 0.1))\\nval adaParams = AdaGradParameters(epochs = 100, learningRate = 0.1, initParams = init)\\n\\n//do the training (argmax is a term, so it needs to be evaluated to do the optimization)\\nval w = argmax(Ws)(w => loss(w).argmaxBy(Argmaxer.adaGrad(adaParams))).eval()\\nw\"]"
      }
    }
  }, {
    "id" : 5,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def pref(w: Ws.Term) = (w.u(1) dot w.u(2))\n\ndef nloss(w: Ws.Term) = loss(w) + pref(w)\n\n//do the training (argmax is a term, so it needs to be evaluated to do the optimization)\nval wn = argmax(Ws)(w => nloss(w).argmaxBy(Argmaxer.adaGrad(adaParams))).eval()\nwn",
      "extraFields" : {
        "cache" : "false",
        "aggregatedCells" : "[\"val K = 2\\nval N = 3\\nval M = 4\\n\\n// set up data\\n@domain case class X(i: Int, j: Int, r: Double)\\nimplicit val Xs = X.Values(Ints(0 until N), Ints(0 until M), Doubles)\\n\\n//the set of possible parameters\\n@domain case class W(u: IndexedSeq[Vect], v: IndexedSeq[Vect])\\nimplicit val Ws = W.Values(Seqs(Vectors(K), N), Seqs(Vectors(K), M))\\n\\nWs.zero.toConst\",\"val input = Map((0 -> 0) -> 1.0, (0 -> 1) -> 1.0, (0 -> 2) -> 0.0, (0 -> 3) -> 0.0,\\n    (1 -> 0) -> 0.0, (1 -> 1) -> 0.0, (1 -> 2) -> 1.0, (1 -> 3) -> 1.0,\\n    (2 -> 0) -> 0.0\\n)\\nval data = input.map(p => X(p._1._1, p._1._2, p._2)).toSet\\nlazy val dataTerm = data.toIndexedSeq.toConst\\n\\n//input\\n//data\\nMatrix((0 until N).map(i => (0 until M).map(j => input.getOrElse(i -> j, 0.5))))\",\"def score(w: Ws.Term)(x: Xs.Term) = w.u(x.i) dot w.v(x.j)\\n\\ndef regularize(w: Ws.Term)(x: Xs.Term) = w.u(x.i).l2() + w.v(x.j).l2()\\n\\nval lambda = 0.1\\n// set up learning\\ndef loss(w: Ws.Term) = sum(dataTerm) {\\n    x => -sq(x.r - score(w)(x)) - lambda*regularize(w)(x)\\n}\\nscore(Ws.zero.toConst)(X(0,0,1.0).toConst)\",\"// visualize results\\nval init = Settings(Ws.createRandomSetting(random.nextGaussian() * 0.1))\\nval adaParams = AdaGradParameters(epochs = 100, learningRate = 0.1, initParams = init)\\n\\n//do the training (argmax is a term, so it needs to be evaluated to do the optimization)\\nval w = argmax(Ws)(w => loss(w).argmaxBy(Argmaxer.adaGrad(adaParams))).eval()\\nw\",\"// predict\\ndef predict(person: Int, movie: Int) = score(w.toConst)(X(person, movie, 0.0).toConst).eval()\\nMatrix((0 until N).map(i => (0 until M).map(j => predict(i, j))))\"]"
      }
    }
  }, {
    "id" : 6,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "// predict\ndef npredict(person: Int, movie: Int) = score(wn.toConst)(X(person, movie, 0.0).toConst).eval()\nMatrix((0 until N).map(i => (0 until M).map(j => npredict(i, j))))",
      "extraFields" : {
        "cache" : "false",
        "aggregatedCells" : "[\"val K = 2\\nval N = 3\\nval M = 4\\n\\n// set up data\\n@domain case class X(i: Int, j: Int, r: Double)\\nimplicit val Xs = X.Values(Ints(0 until N), Ints(0 until M), Doubles)\\n\\n//the set of possible parameters\\n@domain case class W(u: IndexedSeq[Vect], v: IndexedSeq[Vect])\\nimplicit val Ws = W.Values(Seqs(Vectors(K), N), Seqs(Vectors(K), M))\\n\\nWs.zero.toConst\",\"val input = Map((0 -> 0) -> 1.0, (0 -> 1) -> 1.0, (0 -> 2) -> 0.0, (0 -> 3) -> 0.0,\\n    (1 -> 0) -> 0.0, (1 -> 1) -> 0.0, (1 -> 2) -> 1.0, (1 -> 3) -> 1.0,\\n    (2 -> 0) -> 0.0\\n)\\nval data = input.map(p => X(p._1._1, p._1._2, p._2)).toSet\\nlazy val dataTerm = data.toIndexedSeq.toConst\\n\\n//input\\n//data\\nMatrix((0 until N).map(i => (0 until M).map(j => input.getOrElse(i -> j, 0.5))))\",\"def score(w: Ws.Term)(x: Xs.Term) = w.u(x.i) dot w.v(x.j)\\n\\ndef regularize(w: Ws.Term)(x: Xs.Term) = w.u(x.i).l2() + w.v(x.j).l2()\\n\\nval lambda = 0.1\\n// set up learning\\ndef loss(w: Ws.Term) = sum(dataTerm) {\\n    x => -sq(x.r - score(w)(x)) - lambda*regularize(w)(x)\\n}\\nscore(Ws.zero.toConst)(X(0,0,1.0).toConst)\",\"// visualize results\\nval init = Settings(Ws.createRandomSetting(random.nextGaussian() * 0.1))\\nval adaParams = AdaGradParameters(epochs = 100, learningRate = 0.1, initParams = init)\\n\\n//do the training (argmax is a term, so it needs to be evaluated to do the optimization)\\nval w = argmax(Ws)(w => loss(w).argmaxBy(Argmaxer.adaGrad(adaParams))).eval()\\nw\",\"// predict\\ndef predict(person: Int, movie: Int) = score(w.toConst)(X(person, movie, 0.0).toConst).eval()\\nMatrix((0 until N).map(i => (0 until M).map(j => predict(i, j))))\",\"def pref(w: Ws.Term) = (w.u(1) dot w.u(2))\\n\\ndef nloss(w: Ws.Term) = loss(w) + pref(w)\\n\\n//do the training (argmax is a term, so it needs to be evaluated to do the optimization)\\nval wn = argmax(Ws)(w => nloss(w).argmaxBy(Argmaxer.adaGrad(adaParams))).eval()\\nwn\"]"
      }
    }
  } ],
  "config" : {
    "autosave" : "false"
  }
}
